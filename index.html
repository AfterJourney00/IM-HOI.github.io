<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>I'M HOI: Inertia-aware Monocular Capture of 3D Human-Object Interactions</title>
    <!-- Bootstrap -->
    <link href="css/bootstrap-4.4.1.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <!-- <link href='http://fonts.googleapis.com/css?family=Open+Sans:400italic,700italic,800italic,400,700,800' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="css/project.css" media="screen" />
    <link rel="stylesheet" type="text/css" media="screen" href="css/iconize.css" />
    <script src="js/google-code-prettify/prettify.js"></script> -->
    <link rel="icon" type="image/png" sizes="16x16" href="images/favicon_package/favicon-512x512.png">
  </head>

  <!-- cover -->
  <section>
    <div class="jumbotron text-center mt-0">
      <div class="container">
        <div class="row">
          <div class="col-12">
            <h2>I'M HOI: Inertia-aware Monocular Capture of 3D Human-Object Interactions
            </h2>
            <h4 style="color:#ff0000;"><b>CVPR 2024</b></h4>
            <hr>
            <h6>
                <a href="https://afterjourney00.github.io/" target="_blank">Chengfeng Zhao</a><sup>1</sup>,
                <a href="https://juzezhang.github.io/" target="_blank">Juze Zhang</a><sup>1,2,3</sup>,
                <a href="https://alt-js.github.io/" target="_blank">Jiashen Du</a><sup>1</sup>,
                <a href="https://cunkaixin.netlify.app" target="_blank">Ziwei Shan</a><sup>1</sup>,
                Junye Wang<sup>1</sup>,
                <a href="https://scholar.google.com/citations?user=R9L_AfQAAAAJ&hl=en" target="_blank">Jingyi Yu</a><sup>1</sup>,
                <a href="https://faculty.sist.shanghaitech.edu.cn/faculty/wangjingya/" target="_blank">Jingya Wang</a><sup>1</sup>,
                <a href="https://www.xu-lan.com/" target="_blank">Lan Xu</a><sup>1</sup>
            </h6>
            <p>
                <sup>1</sup>ShanghaiTech University &nbsp;&nbsp;
                <sup>2</sup>Shanghai Advanced Research Institute, Chinese Academy of Sciences
                <br>
                <sup>3</sup>University of Chinese Academy of Sciences
            </p>
            <div class="row justify-content-center">
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="https://arxiv.org/abs/2312.08869" role="button"  target="_blank">
                    <i class="fa fa-file"></i> Paper </a> </p>
              </div>
              <div class="column">
                <p class="mb-5"><a class="btn btn-large btn-light" href="https://www.youtube.com/watch?v=MdG00uakBa8" role="button"  target="_blank">
                  <i class="fa fa-youtube"></i> Video </a> </p>
            </div>
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="https://github.com/AfterJourney00/IMHD-Dataset" role="button"  target="_blank">
                    <i class="fa fa-github"></i> GitHub </a> </p>
              </div>
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="https://forms.gle/3MDh3b4szhFwcYa26" role="button"  target="_blank">
                    <i class="fa fa-database"></i> Dataset </a> </p>
              </div>
            </div>

          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- abstract -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
          <h3>Abstract</h3>
            <hr style="margin-top:0px">
            <!-- <h6 style="color:#8899a5"> I'M HOI is able to reconstruct both shape and BRDF of reflective objects using only multiview images. </h6> -->
            <div class="row" style="margin-bottom:5px">
              <div class="col" style="text-align:center">
                <video width="80%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
                  <source src="video/intro.mp4" type="video/mp4">
                </video>
              </div>
            </div>
			      <br>
            <p class="text-left">
              We are living in a world surrounded by diverse and “smart” devices with rich modalities of sensing ability. Conveniently capturing the interactions between us humans and these objects remains far-reaching. In this paper, we present I'm-HOI, a monocular scheme to faithfully capture the 3D motions of both the human and object in a novel setting: using a minimal amount of RGB camera and object-mounted Inertial Measurement Unit (IMU). It combines general motion inference and category-aware refinement. For the former, we introduce a holistic human-object tracking method to fuse the IMU signals and the RGB stream and progressively recover the human motions and subsequently the companion object motions. For the latter, we tailor a category-aware motion diffusion model, which is conditioned on both the raw IMU observations and the results from the previous stage under over-parameterization representation. It significantly refines the initial results and generates vivid body, hand, and object motions. Moreover, we contribute a large dataset with ground truth human and object motions, dense RGB inputs, and rich object-mounted IMU measurements. Extensive experiments demonstrate the effectiveness of I'm-HOI under a hybrid capture setting. Our dataset and code will be released to the community.
            </p>
        </div>
      </div>
    </div>
  </section>
  <br>

  <!-- pipeline -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h2>Pipeline</h2>
            <hr style="margin-top:0px">
            <img src="images/pipeline.png" width="100%">
        </div>
      </div>
      <br>
      <p class="text-left">
        I'm-HOI combines general motion inference and category-aware refinement.
        For the former, we introduce a holistic human-object tracking method to fuse IMU signals with RGB stream, then recover the human and companion object motions progressively.
        For the latter, we tailor a category-aware motion diffusion model, which is conditioned on both the raw IMU observations and the results from the previous stage under over-parameterization representation.
        It significantly refines the initial results and generates vivid body, hand, and object motions.
      </p>
    </div>
  </section>
  <br>

  <!-- dataset -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h2>IMHD Dataset</h2>
            <hr style="margin-top:0px">
            <img src="images/dataset_gallery_full.png" width="100%">
            <video width="80%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
              <source src="video/dataset.mp4" type="video/mp4">
            </video>
        </div>
      </div>
      <br>
      <p class="text-left">
        We exhibit sampled highlights of <i><b>I</b>nertial and <b>M</b>ulti-view <b>H</b>ighly <b>D</b>ynamic human-object interactions <b>D</b>ataset (<b>IMHD<sup>2</sup></b>)</i> on the left side, and 10 well-scanned objects on the right side. In total, our dataset records 295 sequences and captures about 892k frames of data.
      </p>
    </div>
  </section>
  <br>

  <!-- results -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h2>Capture Results</h2>
            <hr style="margin-top:0px">
            <video width="80%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
                <source src="video/comparison_skateboard.mp4" type="video/mp4">
            </video>
            <video width="80%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
              <source src="video/comparison_baseball.mp4" type="video/mp4">
            </video>
            <video width="80%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
              <source src="video/comparison_dumbbell.mp4" type="video/mp4">
            </video>
            <video width="80%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
              <source src="video/comparison_tennis.mp4" type="video/mp4">
            </video>
            <video width="80%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
              <source src="video/comparison_broom.mp4" type="video/mp4">
            </video>
            <video width="80%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
              <source src="video/comparison_chair.mp4" type="video/mp4">
            </video>
            <video width="80%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
              <source src="video/comparison_pan.mp4" type="video/mp4">
            </video>
            <video width="80%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
              <source src="video/comparison_kettlebell.mp4" type="video/mp4">
            </video>
        </div>
      </div>
      <br>
      <p class="text-left">
        I'm-HOI performs consistently better than baselines on multiple datasets, especially on IMHD<sup>2</sup> which is characterized by fast interaction motions.
      </p>
    </div>
  </section>
  <br>

  <!-- citing -->
  <div class="container">
    <div class="row ">
      <div class="col-12">
          <h3>Citation</h3>
          <hr style="margin-top:0px">
              <pre style="background-color: #e9eeef;padding: 1.25em 1.5em">
<code>@InProceedings{zhao2024imhoi,
  author    = {Zhao, Chengfeng and Zhang, Juze and Du, Jiashen and Shan, Ziwei and Wang, Junye and Yu, Jingyi and Wang, Jingya and Xu, Lan},
  title     = {I'M HOI: Inertia-aware Monocular Capture of 3D Human-Object Interactions},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2024},
  pages     = {729-741}
}</code></pre>
          <hr>
      </div>
    </div>
  </div>

  <footer class="text-center" style="margin-bottom:10px">
      Thanks to <a href="https://lioryariv.github.io/" target="_blank">Lior Yariv</a> for the website template.
  </footer>

</body>
</html>
